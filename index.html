<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Michael S. Lee</title>

    <meta name="author" content="Michael S. Lee">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr style="padding:0px">
	  <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr style="padding:0px">
		  <td style="padding:2.5%;width:63%;vertical-align:middle">
		    <p style="text-align:center">
                      <name>Michael S. Lee</name>
		    </p>
		    <p>Hi, I am a final-year PhD student in the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> at <a href="https://www.cmu.edu//">Carnegie Mellon University</a>, working at the intersection of <strong>explainable AI</strong> and <strong>human-robot interaction</strong>. I am fortunate to be advised by <a href="http://www.cs.cmu.edu/~reids/">Reid Simmons</a> and <a href="http://www.hennyadmoni.com/">Henny Admoni</a>, and am part of the <a href="http://harp.ri.cmu.edu/">Human and Robot Partners (HARP) Lab</a>.
		    </p>
		    <p>
		      As AI and robots increasingly enter human society, it is paramount that their reward functions and subsequent behaviors are <em>transparent</em> (i.e. understandable and predictable) to humans. Thus, <strong>I research how robots may intuitively summarize and convey their underlying reward functions (and subsequent policies) to humans using informative demonstrations</strong>.

		      <!-- To collaborate and co-exist fluently with robots, humans must be able to understand their decision making. Thus, <strong>I research how a robot may intuitively summarize and convey its reward function (and subsequent policy) to a human using informative demonstrations</strong>. -->


		    </p>
		    <p>
		      I previously completed a Master's at Carnegie Mellon with <a href="https://www.ri.cmu.edu/ri-faculty/william-red-l-whittaker/">Red Whittaker</a> and <a href="https://www.ri.cmu.edu/ri-faculty/nathan-michael/">Nathan Michael</a> studying autonomous radiation source localization. And before that, I graduated from <a href="https://www.princeton.edu/">Princeton University</a> ('16) with degrees in Mechanical & Aerospace Engineering and a certificate in Robotics & Intelligent Systems. My undergraduate thesis on modeling uncertainty in stereo visual odometry was advised by <a href="https://www.ri.cmu.edu/ri-faculty/nathan-michael/">Nathan Michael</a> and <a href="https://www.linkedin.com/in/jianxiongxiao/">Jianxiong Xiao</a>.
		    </p>

		    <p>
		      <b>I plan to transition into industry in 2024, and am currently looking for opportunities related to my research interests (including but not limited to transparency, value/AI alignment, explainable AI, human-robot interaction).</b>
		    </p>

		    <p style="text-align:center">
                      <a href="mailto:ml5@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                      <a href="data/mlee_cv_alt.pdf">CV</a> &nbsp/&nbsp
                      <a href="https://scholar.google.com/citations?hl=en&user=YGfV85kAAAAJ">Google Scholar</a> &nbsp/&nbsp
                      <a href="https://www.linkedin.com/in/symikelee/">LinkedIn</a>
		    </p>
		  </td>
		  <td style="padding:2.5%;width:40%;max-width:40%">
		    <!-- <img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot_oval.png"> -->
		    <a href="images/headshot.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot_oval.png" class="hoverZoomLink"></a>
		  </td>
		</tr>
            </tbody></table>



	    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr>
		  <td style="padding:20px;width:100%;vertical-align:middle">

		    <heading>Research</heading>


		    <p>
		      Toward greater transparency (i.e. understandability and predictability) robot behaviors to humans, my current research explores how a robot may teach its underlying reward function to a human learner using <strong>informative demonstrations</strong> (i.e. ones that <strong>exhibit key tradeoffs in decision making</strong>).

		      <!-- Toward human understanding of robot decision making, my current research explores how a robot may teach its current reward function to a human learner using <strong>informative demonstrations</strong> (i.e. ones that <strong>exhibit key tradeoffs</strong>). -->
		    </p>
		    <p>
		      Our first key insight is that a demonstration's informativeness to a human is not intrinsic, but is inherently tied to that human's prior beliefs and their current expectations of robot behavior. We thus rely on  <strong>inverse reinforcement learning</strong> and <strong>counterfactual reasoning</strong> (i.e. the extent to which the robot's demonstration differs from the human's current expectations) to evaluate a candidate demonstration's informativeness to a human at revealing the robot's reward function.
		     <!-- We calculate the informativeness of a demonstration and a maintain a model of a human's beliefs by assuming that humans employ a form of <strong>inverse reinforcement learning</strong>. -->
		    </p>
		    <p>
		      Our second key insight is that informativeness and difficulty of comprehension are often two sides of the same coin for humans, and we leverage ideas from cognitive science (e.g. <strong>zone of proximal development / "Goldilocks" principle</strong>) to ensure that the selected demonstrations present the <em>right level of challenge</em>. Too small of a difference and the reconciliation in the human's mind is trivial, and too large of a difference and the gap is irreconcilable in one shot; we use <em>scaffolding</em> to incrementally increase in information gain and simultaneously ease humans into learning.
		    </p>

		    <p>
		      Finally, we are currently exploring how to select a suite of <b>informative tests</b> (which query the human's ability to correctly predict robot behavior unseen scenarios) that reveal and bridge remaining gaps in the human learner's understanding, which can then be further supported through subsequent targeted demonstrations in a <b>closed-loop fashion</b>.
		    </p>

		  </td>
		</tr>
            </tbody></table>




            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		<tr>
		  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/closed_loop.png' width="160">
		  </td>
		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://symikelee.github.io/papers/lee_simmons_ICML_WS2023.pdf">
                      <papertitle>Closed-loop Reasoning about Counterfactuals to Improve Policy Transparency</papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>,
		    <a href="http://www.hennyadmoni.com/">Henny Admoni</a>,
		    <a href="http://www.cs.cmu.edu/~reids/">Reid Simmons</a>
		    <br>
		    <em>ICML Workshop on Counterfactuals in Minds and Machines</em>, 2023
		    <br>
		    <a href="https://symikelee.github.io/papers/lee_simmons_ICML_WS2023.pdf">pdf</a> /
		    <a href="https://symikelee.github.io/papers/ICML_poster_2.pdf">poster</a>
		    <p></p>
		    <p>Design a closed-loop teaching scheme where AI policy is made more transparent to a human via demonstrations, tests, and feedback. Demonstrations are selected to be informative and understandable given a human's counterfactual expectations of the AI's policy.</p>
		  </td>
		</tr>

	    <tr>
		  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/belief_calibration.png' width="160">
		  </td>
		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://symikelee.github.io/papers/zhang_chen_ICML_WS2023.pdf">
                      <papertitle>Leveraging Contextual Counterfactuals Toward Belief Calibration</papertitle>
		    </a>
		    <br>
		    <a href="https://qiuyiz.github.io/">Qiuyi (Richard) Zhang</a>,
		    <strong>Michael S. Lee</strong>,
		    <a href="https://research.google/people/105793/">Sherol Chen</a>
		    <br>
		    <em>ICML Workshop on Counterfactuals in Minds and Machines</em>, 2023
		    <br>
		    <a href="https://symikelee.github.io/papers/zhang_chen_ICML_WS2023.pdf">pdf</a> /
		    <a href="https://symikelee.github.io/papers/ICML_poster_1.pdf">poster</a>
		    <!-- <a href="https://github.com/SUCCESS-MURI/counterfactual_human_IRL_study">user study</a> -->
		    <p></p>
		    <p>Leverage counterfactual reasoning over possible outcomes and recourses to identify optimal belief strengths (i.e. parameters) for algorithmic decision making.</p>
		  </td>
		</tr>

		<tr bgcolor="#ffffd0">
		  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/transparency_research_summary.png' width="160">
		  </td>
		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://symikelee.github.io/papers/research_statement.pdf">
                      <papertitle>Making Robot Policies Transparent to Humans Through Demonstrations</papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>
		    <br>
		    <p></p>
		    <p>My research statement summarizing the work to date and highlighting potential directions for future work.</p>
		  </td>
		</tr>


		<tr>
		  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/IROS2022_counterfactual4.png' width="160">
		  </td>
		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://arxiv.org/abs/2203.01855">
                      <papertitle>Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning</papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>,
		    <a href="http://www.hennyadmoni.com/">Henny Admoni</a>,
		    <a href="http://www.cs.cmu.edu/~reids/">Reid Simmons</a>
		    <br>
		    <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
		    <br>
		    <a href="https://arxiv.org/pdf/2203.01855.pdf">pdf</a> /
		    <a href="https://github.com/SUCCESS-MURI/counterfactual_human_IRL">code</a> /
		    <a href="https://github.com/SUCCESS-MURI/counterfactual_human_IRL_study">user study</a>
		    <p></p>
		    <p>An informative demonstration is one that differs strongly from the learner‚Äôs expectations of what the robot will do given their current understanding of the robot‚Äôs decision making.</p>
		  </td>
		</tr>

		<tr>
		  <td style="padding:20px;width:25%;vertical-align:middle">
		    <img src='images/BT.png' width="160">
		  </td>
		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://dl.acm.org/doi/abs/10.1145/3457185">
		      <papertitle>Building the Foundation of Robot Explanation Generation Using Behavior Trees</papertitle>
		    </a>
		    <br>
		    <a href="https://zhaohanphd.com/">Zhao Han</a>,
		    <a href="https://www.linkedin.com/in/daniel-giger/">Daniel Giger</a>,
		    <a href="https://www.linkedin.com/in/jordan-allspaw-38b582197/">Jordan Allspaw</a>,
		    <a href="http://www.hennyadmoni.com/">Henny Admoni</a>,
		    <strong>Michael S. Lee</strong>,
		    <a href="https://www.uml-hri-lab.com/holly">Holly Yanco</a>
		    <br>
		    <em>ACM Transactions on Human-Robot Interaction (THRI)</em>, 2022
		    <br>
		    <a href="http://harp.ri.cmu.edu/assets/pubs/lee_THRI2021.pdf">pdf</a> /
		    <a href="https://github.com/uml-robotics/robot-explanation-BTs">code</a> /
		    <a href="https://zhaohanphd.com/publications/thri21a-building-the-foundation-of-robot-explanation-generation-using-behavior-trees/">project page</a> /
		    <a href="https://cs.uml.edu/~zhan/thri-bt/Robot%20Explanation%20with%20BT%20(Invited%20Talk%20at%20Georgia%20Tech).pdf">slides</a>
		    <p></p>
		    <p>A behavior tree hierarchically composed of goals, subgoals, steps, and actions supports explanation generation algorithms that convey causal information about robot behavior.</p>
		  </td>
		</tr>


		<tr onmouseout="sirfs_stop()" onmouseover="sirfs_start()">
		  <td style="padding:20px;width:25%;vertical-align:middle">
		    <div class="one" style="display:flex;align-items:center">
                      <div class="two" id='sirfs_image' style="display:flex;align-items:center"><img src='images/Frontiers2021_b.png' width="160"></div>
                      <img src='images/Frontiers2021_a.png' width="160">
		    </div>
		    <script type="text/javascript">
                      function sirfs_start() {
                      document.getElementById('sirfs_image').style.opacity = "1";
                      }

                      function sirfs_stop() {
                      document.getElementById('sirfs_image').style.opacity = "0";
                      }
                      sirfs_stop()
		    </script>
		  </td>

		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://www.frontiersin.org/articles/10.3389/frobt.2021.693050/full">
                      <papertitle>Machine Teaching for Human Inverse Reinforcement Learning</papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>,
		    <a href="http://www.hennyadmoni.com/">Henny Admoni</a>,
		    <a href="http://www.cs.cmu.edu/~reids/">Reid Simmons</a>
		    <br>
		    <em>Frontiers in Robotics and AI</em>, 2021
		    <br>
		    <a href="http://harp.ri.cmu.edu/assets/pubs/lee_frontiers21.pdf">pdf</a>
		    /
		    <a href="https://github.com/SUCCESS-MURI/machine-teaching-human-IRL">code</a>
		    /
		    <a href="https://github.com/SUCCESS-MURI/psiturk-machine-teaching">user study</a>
		    <p></p>
		    <p> Augmenting an inverse reinforcement learning model of how humans learn from demonstrations with teaching strategies (e.g. scaffolding, simplicity, pattern discovery, and testing) better accommodates human learners.
		    </p>
		  </td>
		</tr>


		<tr>
		  <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='images/baxter.png' width="160">
		  </td>
		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://ieeexplore.ieee.org/abstract/document/8673083">
                      <papertitle>Self-Assessing and Communicating Manipulation Proficiency Through Active Uncertainty Characterization</papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>,
		    <br>
		    <em>Pioneers Workshop at ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>, 2019
		    <br>
		    <a href="http://harp.ri.cmu.edu/assets/pubs/lee_hri_pioneers_2019.pdf">pdf</a>
		    <p></p>
		    <p>This proposal thus investigates how a robot can actively assess both its proficiency at a task and its confidence in that assessment through appropriate measures of uncertainty that can be efficiently and effectively communicated to a human.</p>
		  </td>
		</tr>
            </tbody></table>







	    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr>
		  <td style="padding:20px;width:100%;vertical-align:middle">
		    <!-- <heading>Research2</heading> -->
		    <p>
		      I previously developed a novel gamma radiation map representation, source localization algorithm, and frontier-based exploration for efficient radiological characterization of nuclear facilities using a robot equipped with a gamma-ray camera.
		    </p>
		  </td>
		</tr>
            </tbody></table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




		<tr onmouseout="blocknerf_stop()" onmouseover="blocknerf_start()">
		  <td style="padding:20px;width:25%;vertical-align:middle">
		    <div class="one" style="display:flex;align-items:center">
                      <div class="two" id='blocknerf_image'><video  width=100% height=100% muted autoplay loop>
			  <source src="images/e1_cropped_spedup_cropped_further.mp4" type="video/mp4">
			  Your browser does not support the video tag.
                      </video></div>
                      <img src='images/frontier_exploration.png' width="160" style="vertical-align:middle">
		    </div>
		    <script type="text/javascript">
                      function blocknerf_start() {
			  document.getElementById('blocknerf_image').style.opacity = "1";
                      }

                      function blocknerf_stop() {
			  document.getElementById('blocknerf_image').style.opacity = "0";
                      }
                      blocknerf_stop()
		    </script>
		  </td>
		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://www.ri.cmu.edu/publications/radiation-source-localization-using-a-gamma-ray-camera/">
                      <papertitle>Radiation Source Localization using a Gamma-ray Camera
		      </papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>
		    <br>
		    <em>Master's Thesis</em>, 2018
		    <br>
		    <a href="data/ml5_thesis.pdf">pdf</a>
		    /
		    <a href="https://docs.google.com/presentation/d/1iZ2I0YyuaN0-t3L-G4ey1t7fBoJNJ0Vcg3bqMf34HCo/edit#slide=id.g3df372ab48_0_0">slides</a>
		    <p></p>
		    <p>The proposed frontier-based exploration method biases frontier selection with the observed radiation field gradient to quickly search an environment until a proximal source is detected.</p>
		  </td>
		</tr>

		<tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">

		  <td style="padding:20px;width:25%;vertical-align:middle">
		    <div class="one" style="display:flex;align-items:center">
                      <div class="two" id='hnerf_image' style="display:flex;align-items:center"><img src='images/IROS2018b.png' width="160"></div>
                      <img src='images/IROS2018a.png' width="160">
		    </div>
		    <script type="text/javascript">
                      function hnerf_start() {
                      document.getElementById('hnerf_image').style.opacity = "1";
                      }

                      function hnerf_stop() {
                      document.getElementById('hnerf_image').style.opacity = "0";
                      }
                      hnerf_stop()
		    </script>
		  </td>


		  <td style="padding:20px;width:75%;vertical-align:middle">
		    <a href="https://ieeexplore.ieee.org/abstract/document/8593625">
                      <papertitle>Active Range and Bearing-based Radiation Source Localization</papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>,
		    <a href="https://www.danielshy.com/home">Daniel Shy</a>,
		    <a href="https://www.ri.cmu.edu/ri-faculty/william-red-l-whittaker/">Red Whittaker</a>,
		    <a href="https://www.ri.cmu.edu/ri-faculty/nathan-michael/">Nathan Michael</a>
		    <br>
		    <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2018
		    <br>
		    <a href="data/lee_michael_IROS2018.pdf">pdf</a>
		    /
		    <a href="https://docs.google.com/presentation/d/1FGnnkbu8i7oXstvs5O710jvqcDlzPTd86GNd-WlnC_U/edit#slide=id.g3f70fb0657_2_212">slides</a>
		    <p></p>
		    <p>The proposed active source localization algorithm greedily selects new waypoints that maximize the Fisher Information provided by the gamma-ray camera‚Äôs range and bearing observations.</p>
		  </td>
		</tr>

		<tr onmouseout="urfnerf_stop()" onmouseover="urfnerf_start()">

		  <td style="padding:20px;width:25%;vertical-align:middle">
		    <div class="one" style="display:flex;justify-content:center;align-items:center">
                      <div class="two" id='urf_image'><video  width=100% height=100% muted autoplay loop>
			  <source src="images/umich_compressed.mp4" type="video/mp4">
			  Your browser does not support the video tag.
                      </video></div>
                      <img src='images/WM2018_cropped.png' width="160" style="vertical-align:middle;horizontal-align:middle">
		    </div>
		    <script type="text/javascript">
                      function urfnerf_start() {
                      document.getElementById('urf_image').style.opacity = "1";
                      }

                      function urfnerf_stop() {
                      document.getElementById('urf_image').style.opacity = "0";
                      }
                      urfnerf_stop()
		    </script>
		  </td>

		  <td style="padding:20px;width:75%;vertical-align:midwdle">
		    <a href="https://arxiv.org/abs/1802.06072">
                      <papertitle>3-D Volumetric Gamma-ray Imaging and Source Localization with a Mobile Robot</papertitle>
		    </a>
		    <br>
		    <strong>Michael S. Lee</strong>
		    <a href="https://www.linkedin.com/in/matt-hanczor/">Matthew Hanczor</a>,
		    <a href="https://www.linkedin.com/in/jiyang-chu/">Jiyang Chu</a>,
		    <a href="https://ners.engin.umich.edu/people/he-zhong/">Zhong He</a>,
		    <a href="https://www.ri.cmu.edu/ri-faculty/nathan-michael/">Nathan Michael</a>,
		    <a href="https://www.ri.cmu.edu/ri-faculty/william-red-l-whittaker/">Red Whittaker</a>
		    <br>
		    <em>Waste Management Conference</em>, 2018
		    <br>
		    <a href="https://arxiv.org/pdf/1802.06072.pdf">pdf</a>
		    /
		    <a href="https://docs.google.com/presentation/d/18QEj1YcvujGirfruD6R07mPzxJCJUuZv7qMAou76DPc/edit?usp=sharing">slides</a>
		    <p></p>
		    <p>
		      A ground robot equipped with a Compton gamma camera localizes multiple gamma radiation sources to within an average of 0.26 m or 0.6% of the environment dimensions in two 5√ó4 m<sup>2</sup> and 14√ó6 m<sup>2</sup> laboratory environments.</p>
		  </td>
		</tr>
            </tbody></table>



            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody> -->
		<!-- 	<tr> -->
		  <!-- 	  <td> -->
		    <!-- 	    <heading>Misc</heading> -->
		    <!-- 	  </td> -->
		  <!-- 	</tr> -->
		<!-- </tbody></table> -->
            <!-- <table width="100%" align="center" border="0" cellpadding="20"><tbody> -->

		<!-- 	<tr> -->
		  <!-- 	  <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td> -->
		  <!-- 	  <td width="75%" valign="center"> -->
		    <!-- 	    <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a> -->
		    <!-- 	    <br> -->
		    <!-- 	    <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a> -->
		    <!-- 	    <br> -->
		    <!-- 	    <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a> -->
		    <!-- 	    <br> -->
		    <!-- 	    <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a> -->
		    <!-- 	  </td> -->
		  <!-- 	</tr> -->
		<!-- 	<tr> -->
		  <!-- 	  <td style="padding:20px;width:25%;vertical-align:middle"> -->
		    <!-- 	    <img src="images/cs188.jpg" alt="cs188"> -->
		    <!-- 	  </td> -->
		  <!-- 	  <td width="75%" valign="center"> -->
		    <!-- 	    <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a> -->
		    <!-- 	    <br> -->
		    <!-- 	    <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a> -->
		    <!-- 	    <br> -->
		    <!-- 	    <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a> -->
		    <!-- 	  </td> -->
		  <!-- 	</tr> -->


		<!-- 	<tr> -->
		  <!-- 	  <td align="center" style="padding:20px;width:25%;vertical-align:middle"> -->
		    <!-- 	    <heading>Basically <br> Blog Posts</heading> -->
		    <!-- 	  </td> -->
		  <!-- 	  <td width="75%" valign="middle"> -->
		    <!-- 	    <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a> -->
		    <!-- 	    <br> -->
		    <!-- 	    <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a> -->
		    <!-- 	    <br> -->
		    <!-- 	    <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a> -->
		    <!-- 	  </td> -->
		  <!-- 	</tr> -->


		<!-- </tbody></table> -->



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr>
		  <td style="padding:0px">
		    <br>
		    <p style="text-align:center;font-size:small;">
                      <br>
                      Design by <a href="https://jonbarron.info/">Jon Barron</a>.
		    </p>
		  </td>
		</tr>
            </tbody></table>
	  </td>
	</tr>
    </table>
  </body>

</html>
